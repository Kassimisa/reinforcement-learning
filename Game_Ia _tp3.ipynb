{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f48c8f32-4110-4cfb-a2b1-da0514899ad8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Premières lignes de policy_table:\n",
      "[[0.16666667 0.16666667 0.16666667 0.16666667 0.16666667 0.16666667]\n",
      " [0.16666667 0.16666667 0.16666667 0.16666667 0.16666667 0.16666667]\n",
      " [0.16666667 0.16666667 0.16666667 0.16666667 0.16666667 0.16666667]\n",
      " [0.16666667 0.16666667 0.16666667 0.16666667 0.16666667 0.16666667]\n",
      " [0.16666667 0.16666667 0.16666667 0.16666667 0.16666667 0.16666667]]\n",
      "\n",
      "Premières lignes de value_table:\n",
      "[0. 0. 0. 0. 0.]\n"
     ]
    }
   ],
   "source": [
    "import gymnasium as gym\n",
    "import numpy as np \n",
    "env = gym.make(\"Taxi-v3\",render_mode=\"human\")\n",
    "state_size = env.observation_space.n\n",
    "action_size= env.action_space.n\n",
    "#******************table de politique***\n",
    "policy_table = np.ones((state_size, action_size))/ action_size\n",
    "\n",
    "value_table = np.zeros(state_size)\n",
    "\n",
    "print(\"Premières lignes de policy_table:\")\n",
    "print(policy_table[:5])\n",
    "print(\"\\nPremières lignes de value_table:\")\n",
    "print(value_table[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d00b3a16-d938-47e7-872f-faf813dcb916",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Épisode 1\n",
      "-----------------\n"
     ]
    }
   ],
   "source": [
    "def run_random_agent(env, num_episodes=20):\n",
    "    for episode in range(num_episodes):\n",
    "        state = env.reset()\n",
    "        done = False\n",
    "        total_reward = 0\n",
    "        actions = []\n",
    "        rewards = []\n",
    "        \n",
    "        print(f\"\\nÉpisode {episode + 1}\")\n",
    "        print(\"-----------------\")\n",
    "        \n",
    "        while not done:\n",
    "            action = env.action_space.sample()\n",
    "            next_state, reward, done, _, _ = env.step(action)\n",
    "            \n",
    "            actions.append(action)\n",
    "            rewards.append(reward)\n",
    "            total_reward += reward\n",
    "            state = next_state\n",
    "        \n",
    "        print(\"Actions exécutées:\", actions)\n",
    "        print(\"Récompenses obtenues:\", rewards)\n",
    "        print(\"Récompense totale:\", total_reward)\n",
    "\n",
    "run_random_agent(env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f354671d-6e7b-4786-b600-f12d857ecfd9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mise à jour PPO terminée avec succès!\n",
      "Nombre d'étapes dans l'épisode: 755\n",
      "Récompense totale: -3020\n",
      "Exemple de mise à jour - État 0:\n",
      "Ancienne politique: [0.167 0.167 0.167 0.167 0.167 0.167]\n",
      "Nouvelle valeur: 0.000\n"
     ]
    }
   ],
   "source": [
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "\n",
    "env = gym.make(\"Taxi-v3\",render_mode=\"human\")\n",
    "state_size = env.observation_space.n\n",
    "action_size = env.action_space.n\n",
    "\n",
    "policy_table = np.full((state_size, action_size), 1/action_size)  \n",
    "value_table = np.zeros(state_size)  \n",
    "\n",
    "gamma = 0.99\n",
    "lr_policy = 0.1\n",
    "clip_epsilon = 0.2\n",
    "\n",
    "def calculate_discounted_rewards(rewards):\n",
    "    discounted = np.zeros_like(rewards, dtype=np.float32)\n",
    "    running_add = 0\n",
    "    for t in reversed(range(len(rewards))):\n",
    "        running_add = running_add * gamma + rewards[t]\n",
    "        discounted[t] = running_add\n",
    "    return discounted\n",
    "\n",
    "state = env.reset()[0]\n",
    "done = False\n",
    "episode_states = []\n",
    "episode_actions = []\n",
    "episode_rewards = []\n",
    "\n",
    "while not done:\n",
    "    action_probs = policy_table[state] + 1e-8  \n",
    "    action_probs /= np.sum(action_probs)       \n",
    "    action = np.random.choice(action_size, p=action_probs)\n",
    "    \n",
    "    next_state, reward, done, _, _ = env.step(action)\n",
    "    \n",
    "    episode_states.append(state)\n",
    "    episode_actions.append(action)\n",
    "    episode_rewards.append(reward)\n",
    "    state = next_state\n",
    "\n",
    "discounted_rewards = calculate_discounted_rewards(episode_rewards)\n",
    "advantages = discounted_rewards - value_table[episode_states]  # At = Rt - V(st)\n",
    "\n",
    "for t in range(len(episode_states)):\n",
    "    state = episode_states[t]\n",
    "    action = episode_actions[t]\n",
    "    \n",
    "    old_prob = policy_table[state, action]\n",
    "    ratio = policy_table[state, action] / old_prob\n",
    "    \n",
    "    surr1 = ratio * advantages[t]\n",
    "    surr2 = np.clip(ratio, 1-clip_epsilon, 1+clip_epsilon) * advantages[t]\n",
    "    policy_loss = -np.minimum(surr1, surr2)\n",
    "    \n",
    "    policy_table[state, action] -= lr_policy * policy_loss\n",
    "    value_table[state] += lr_policy * advantages[t]\n",
    "\n",
    "print(\"Mise à jour PPO terminée avec succès!\")\n",
    "print(f\"Nombre d'étapes dans l'épisode: {len(episode_states)}\")\n",
    "print(f\"Récompense totale: {sum(episode_rewards)}\")\n",
    "print(\"Exemple de mise à jour - État 0:\")\n",
    "print(f\"Ancienne politique: {np.round(policy_table[0], 3)}\")\n",
    "print(f\"Nouvelle valeur: {value_table[0]:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9942e795-ff45-4fbd-9ab1-22cf1a3903ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercice 4 : Évaluation de l'agent après entraînement\n",
    "\n",
    "num_eval_episodes = 20  \n",
    "total_rewards = []\n",
    "\n",
    "print(\"\\nDébut .....\")\n",
    "\n",
    "for ep in range(num_eval_episodes):\n",
    "    state = env.reset()[0]\n",
    "    done = False\n",
    "    episode_reward = 0\n",
    "    \n",
    "    while not done:\n",
    "        action = np.argmax(policy_table[state])\n",
    "        next_state, reward, done, _, _ = env.step(action)\n",
    "        episode_reward += reward\n",
    "        state = next_state\n",
    "    \n",
    "    total_rewards.append(episode_reward)\n",
    "    print(f\"Épisode {ep + 1}: Récompense = {episode_reward}\")\n",
    "\n",
    "avg_reward = np.mean(total_rewards)\n",
    "std_reward = np.std(total_rewards)\n",
    "\n",
    "print(\"\\nRésultats finaux:\")\n",
    "print(f\"Récompense moyenne sur {num_eval_episodes} épisodes: {avg_reward:.2f}\")\n",
    "print(f\"Écart-type: {std_reward:.2f}\")\n",
    "\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.bar(['Avant entraînement', 'Après entraînement'], \n",
    "        [-8, avg_reward],  \n",
    "        color=['red', 'green'])\n",
    "plt.title(\"Comparaison des performances\")\n",
    "plt.ylabel(\"Récompense moyenne\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4765d24e-134b-4192-9dae-684f3470d072",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "20c27007-c73f-43ce-a243-258cb1cd5498",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbb0d3df-497b-45fc-a675-27e6bbd14beb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
